{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-cohere langchain-community langchain-core==0.2.40 langchain-openai langchain-qdrant\n",
    "%pip install -qU docx2txt\n",
    "%pip install -qU IProgress\n",
    "%pip install -qU ipywidgets\n",
    "%pip install -qU pymupdf\n",
    "%pip install -qU python-dotenv\n",
    "%pip install -qU ragas==0.1.20\n",
    "%pip install -qU tqdm\n",
    "%pip install -qU unstructured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get environment variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader \n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "def process_directory(path: str, glob: str, loader_cls: str, use_multithreading=True):\n",
    "\t\n",
    "\tloader = DirectoryLoader(path=path, glob=glob, show_progress=True, loader_cls=loader_cls, use_multithreading=use_multithreading)\n",
    "\t\n",
    "\tdocs = loader.load()\n",
    "\t\n",
    "\treturn docs\n",
    "\n",
    "\n",
    "def test_process_directory():\n",
    "\tdocs = []\n",
    "\t\n",
    "\tdocs_pdf = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\t\n",
    "\tdocs.extend(docs_pdf)\n",
    "\tprint(len(docs))\n",
    "\n",
    "test_process_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using OpenAI\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def create_embeddings_openai(model='text-embedding-ada-002') -> OpenAIEmbeddings:\n",
    "\n",
    "\t# Initialize the OpenAIEmbeddings class\n",
    "\tembeddings = OpenAIEmbeddings(model=model)\n",
    "\n",
    "\treturn embeddings\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_embeddings_openai():\n",
    "\ttext = 'What is the annual revenue of Uber?'\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tvector = embeddings.embed_query(text)\n",
    "\tprint(vector)\n",
    "\treturn embeddings\n",
    "\n",
    "test_create_embeddings_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text splitter using recursive character text splitter\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_docs_recursive(docs: list, chunk_size=500, chunk_overlap=50) -> list:\n",
    "\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "\tchunks_start = text_splitter.split_documents(docs)\n",
    "\n",
    "\t# chunks_end = remove_empty_chunks(chunks_start=chunks_start)\n",
    "\n",
    "\treturn chunks_start\n",
    "\n",
    "#####\n",
    "\n",
    "def test_chunk_docs_recursive(): \n",
    "\tdocs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\n",
    "\tprint(f'\\nNumber of chunks = {len(chunks)}\\n')\n",
    "\tprint(f'First chunk = {chunks[0].page_content}')\n",
    "\n",
    "test_chunk_docs_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant vector store\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "def create_qdrant_vector_store(location: str, \n",
    "\t\t\t\t\t\t\t   collection_name: str, \n",
    "\t\t\t\t\t\t\t   vector_size: int, \n",
    "\t\t\t\t\t\t\t   embeddings: Embeddings, \n",
    "\t\t\t\t\t\t\t   docs: list) -> QdrantVectorStore:\n",
    "\n",
    "\t# Initialize the Qdrant client\n",
    "\tqdrant_client = QdrantClient(location=location)\n",
    "\n",
    "\t# Create a collection in Qdrant\n",
    "\tqdrant_client.create_collection(collection_name=collection_name, \n",
    "\t\t\t\t\t\t\t\t vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE))\n",
    "\n",
    "\t# Initialize QdrantVectorStore with the Qdrant client\n",
    "\tqdrant_vector_store = QdrantVectorStore(client=qdrant_client, \n",
    "\t\t\t\t\t\t\t\t\t\t collection_name=collection_name, embedding=embeddings)\n",
    "\t\n",
    "\t# Add the docs to the vector store\n",
    "\tqdrant_vector_store.add_documents(docs)\n",
    "\t\n",
    "\treturn qdrant_vector_store\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_qdrant_vector_store():\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tdocs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\tprint(f'\\nNumber of chunks = {len(chunks)}\\n')\n",
    "\tvector_store = create_qdrant_vector_store(':memory:', 'holiday-test', 1536, embeddings, chunks)\n",
    "\tprint(vector_store.collection_name)\n",
    "\n",
    "test_create_qdrant_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant retriever\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def create_retriever_qdrant(vector_store: QdrantVectorStore) -> BaseRetriever:\n",
    "\n",
    "\tretriever = vector_store.as_retriever()\n",
    "\n",
    "\treturn retriever\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_retriever_qdrant(text: str = None):\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tdocs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\tprint(f'\\nNumber of chunks = {len(chunks)}\\n')\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tif text:\n",
    "\t\tdocs = retriever.invoke(text)\n",
    "\t\tprint(docs[0])\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_retriever_qdrant('What is the annual revenue for Uber?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "\n",
    "# https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/#chatprompttemplate\n",
    "# https://python.langchain.com/v0.2/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_chat_prompt_template(template: str = None) -> ChatPromptTemplate:\n",
    "\t\n",
    "\tif template is None:\n",
    "\t\ttemplate = '''\n",
    "\t\tYou are an expert assistant designed to help users analyze and answer questions about 10K annual reports filed by publicly traded companies. Users may ask about specific sections, financial metrics, trends, or comparisons across multiple reports. Your role is to provide accurate, concise, and relevant answers, referencing appropriate sections or data points where applicable.\n",
    "\n",
    "\t\tWhen responding, adhere to the following principles:\n",
    "\n",
    "\t\tUnderstand the Question: Identify whether the query is focused on a specific company, year, or metric, or if it spans multiple reports for comparison.\n",
    "\t\tClarify Uncertainty: If a user's question is unclear, ask for clarification or additional context.\n",
    "\t\tLocate and Reference Information: Use relevant sections of the 10K report(s), such as MD&A, Financial Statements, Risk Factors, or Notes to Financial Statements, to back up your answers.\n",
    "\t\tSynthesize Data: Provide summaries or insights when the question involves comparing data or trends across multiple reports.\n",
    "\t\tStay Objective: Avoid providing subjective opinions or interpretations beyond the factual content in the reports.\n",
    "\t\tExample User Queries and Expected Responses:\n",
    "\n",
    "\t\t\"What was the revenue for Company X in 2022 and 2023?\"\n",
    "\n",
    "\t\tLocate and report the revenue figures from the Income Statements of the respective 10K reports for 2022 and 2023.\n",
    "\t\t\"What are the main risk factors for Company Y in its latest report?\"\n",
    "\n",
    "\t\tSummarize the key risk factors from the most recent 10K report's \"Risk Factors\" section.\n",
    "\t\t\"How did the operating income of Company Z change over the last three years?\"\n",
    "\n",
    "\t\tExtract operating income figures from the 10K reports for the past three years and provide a brief comparison.\n",
    "\t\t\"Compare the debt levels of Company A and Company B in 2023.\"\n",
    "\n",
    "\t\tRetrieve debt-related figures from the Balance Sheets or Notes to Financial Statements of both companies and summarize the comparison.\n",
    "\t\t\"What trends are evident in Company W's R&D expenses over the last five years?\"\n",
    "\n",
    "\t\tSummarize trends using data from the Income Statements or footnotes for R&D expenses across five consecutive 10K reports.\n",
    "\t\tAssumptions and Constraints:\n",
    "\n",
    "\t\tOnly use inforomation from 10K reports provided in the context below. \n",
    "\t\tFor complex queries spanning multiple reports, provide a structured summary highlighting key comparisons or trends.\n",
    "\t\tIf certain information is unavailable, state so clearly and suggest alternative approaches to obtain it.\n",
    "\n",
    "\t\tNow it's your turn!\n",
    "\t\t\n",
    "\t\t{question}\n",
    "\n",
    "\t\t{context}\n",
    "\t\t'''\n",
    "\t\n",
    "\tprompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\treturn prompt\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chat_prompt_template():\n",
    "\tprompt = create_chat_prompt_template()\n",
    "\tprint(prompt)\n",
    "\n",
    "test_create_chat_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Langchain chain..\n",
    "\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableSerializable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "\n",
    "def create_chain (model: str, \n",
    "\t\t\t\t  prompt_template: ChatPromptTemplate, \n",
    "\t\t\t\t  retriever: BaseRetriever) -> RunnableSerializable:\n",
    "\n",
    "\tllm = ChatOpenAI(model=model)\n",
    "\t\t\n",
    "\tchain = (\n",
    "\t\t{\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")} \n",
    "\t\t| RunnablePassthrough.assign(context=itemgetter(\"context\")) \n",
    "\t\t| {\"response\": prompt_template | llm, \"context\": itemgetter(\"context\")}\n",
    "\t\t)\n",
    "\n",
    "\treturn chain\n",
    "\n",
    "#####\n",
    "\n",
    "def test_create_chain_qdrant():\n",
    "\tembeddings = create_embeddings_openai()\n",
    "\tdocs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "\tchunks = chunk_docs_recursive(docs=docs)\n",
    "\tprint(f'\\nNumber of chunks = {len(chunks)}\\n')\n",
    "\tvector_store = create_qdrant_vector_store(\":memory:\", \"holiday-test\", 1536, embeddings, chunks)\n",
    "\tretriever = create_retriever_qdrant(vector_store)\n",
    "\tchat_prompt_template = create_chat_prompt_template()\n",
    "\tchain = create_chain('gpt-4o', chat_prompt_template, retriever)\n",
    "\tresult = chain.invoke({'question' : 'What is the annual revenue of Uber?'})\n",
    "\tprint(result)\n",
    "\n",
    "print('\\nQDRANT')\n",
    "test_create_chain_qdrant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/\n",
    "# I'm using an older version of Ragas because I couldn't the current version to work \n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas.testset.generator import RunConfig\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "\n",
    "# Split the documents into chunks\n",
    "docs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "chunks = chunk_docs_recursive(docs=docs, chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create the generator \n",
    "generator = TestsetGenerator.from_langchain(generator_llm=generator_llm, critic_llm=critic_llm, embeddings=embeddings)\n",
    "\n",
    "# https://docs.ragas.io/en/v0.1.21/howtos/customisations/run_config.html\n",
    "# Default is 16 but using a smaller number is used to avoid rate limits\n",
    "run_config=RunConfig(max_workers=8)\n",
    "\n",
    "# Set the number of questions\n",
    "test_size=20\n",
    "\n",
    "# Set the distribution \n",
    "distributions = {simple: 0.0, multi_context: 1.0, reasoning: 0.0}\n",
    "\n",
    "# Generate the testset and save to disk \n",
    "testset = generator.generate_with_langchain_docs(documents=docs, test_size=test_size, distributions=distributions, run_config=run_config)\n",
    "\n",
    "# Write the testet to disk\n",
    "testset_name = \"10k_multi_context_testset.csv\"\n",
    "print(f\"Saving {testset_name}\")\n",
    "testset_df = testset.to_pandas()\n",
    "testset_df.to_csv(f\"{testset_name}\")\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Simple RAG Chain Using Qdrant and PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain using Vertex AI Agent Builder datastore\n",
    "\n",
    "embeddings = create_embeddings_openai()\n",
    "docs = process_directory('docs/', '**/*.pdf', PyMuPDFLoader, True)\n",
    "chunks = chunk_docs_recursive(docs=docs)\n",
    "print(f'\\nNumber of chunks = {len(chunks)}\\n')\n",
    "vector_store = create_qdrant_vector_store(':memory:', 'holiday-test', 1536, embeddings, chunks)\n",
    "retriever = create_retriever_qdrant(vector_store)\n",
    "chat_prompt_template = create_chat_prompt_template()\n",
    "chain = create_chain('gpt-4o', chat_prompt_template, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What is the annual revenue of Uber?\",\n",
    "\"What is the annual revenue of Lyft?\",\n",
    "\"How does Uber's revenue compare to Lyft's revenue?\",]\n",
    "\n",
    "for question in questions:\n",
    "\tprint(question)\n",
    "\tresult = chain.invoke({\"question\" : question})\n",
    "\tprint(result)\n",
    "\tprint(result[\"response\"].content)\n",
    "\tprint(\"\\n*****\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "era4-holiday",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
